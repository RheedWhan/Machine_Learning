{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RheedWhan/Machine_Learning/blob/main/MACHINE_LEARNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QWXY8kwygyf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM8L9qXcsxrg"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Divh8XszL1"
      },
      "outputs": [],
      "source": [
        "def load_housing_data():\n",
        "    tarball_path = Path(\"datasets/housing.tgz\")\n",
        "    if not tarball_path.is_file():\n",
        "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
        "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
        "        urllib.request.urlretrieve(url, tarball_path)\n",
        "        with tarfile.open(tarball_path) as housing_tarball:\n",
        "            housing_tarball.extractall(path=\"datasets\")\n",
        "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE21VoL8szyw"
      },
      "outputs": [],
      "source": [
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmma74D8s0Do"
      },
      "outputs": [],
      "source": [
        "housing.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZCO2iuhs0fz"
      },
      "outputs": [],
      "source": [
        "housing.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQZvEAd4u_zM"
      },
      "outputs": [],
      "source": [
        "housing['ocean_proximity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYCy9iwKpgU-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOMAlwgvu_uF"
      },
      "outputs": [],
      "source": [
        "housing.hist(bins=50, figsize=(16,12))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk7Fjyiau_nC"
      },
      "outputs": [],
      "source": [
        "# Create a Test Set\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def shuffle_and_split_data(data, test_ratio):\n",
        "  shuffled_indices = np.random.permutation(len(data))\n",
        "  test_set_size = int(len(data) * test_ratio)\n",
        "  test_indices = shuffled_indices[:test_set_size]\n",
        "  train_indices = shuffled_indices[test_set_size:]\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCdMSIFLu_g0"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
        "\n",
        "len(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clYS--2Gu_XV"
      },
      "outputs": [],
      "source": [
        "len(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAeUKDdSu_KN"
      },
      "outputs": [],
      "source": [
        "from zlib import crc32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5_bbtzSuVf_"
      },
      "outputs": [],
      "source": [
        "def is_id_in_test_set(identifier, test_ratio):\n",
        "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
        "\n",
        "def split_data_with_id_hash(data, test_ratio, id_column):\n",
        "    ids = data[id_column]\n",
        "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
        "    return data.loc[~in_test_set], data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5cMKytNeTDe"
      },
      "outputs": [],
      "source": [
        "housing_with_id = housing.reset_index()  # adds an `index` column\n",
        "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVD25hoikGCW"
      },
      "outputs": [],
      "source": [
        "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
        "train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, \"id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cvMpD4CkkCz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbXw9mMAkxPs"
      },
      "outputs": [],
      "source": [
        "test_set[\"total_bedrooms\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttqNbB9Hk282"
      },
      "outputs": [],
      "source": [
        "# extra code â€“ shows another way to estimate the probability of bad sample\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "sample_size = 1000\n",
        "ratio_female = 0.511\n",
        "\n",
        "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
        "((samples < 485) | (samples > 535)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IzIrUskTjgk"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akUUv6CvUFXp"
      },
      "outputs": [],
      "source": [
        "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True)\n",
        "plt.xlabel(\"Income category\")\n",
        "plt.ylabel(\"Number of districts\")\n",
        "plt.savefig(\"housing_income_cat_bar_plot\")  # extra code\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awLutgHsVXIy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n",
        "strat_splits = []\n",
        "for train_index, test_index in splitter.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set_n = housing.iloc[train_index]\n",
        "    strat_test_set_n = housing.iloc[test_index]\n",
        "    strat_splits.append([strat_train_set_n, strat_test_set_n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG8--gzo4sBz"
      },
      "outputs": [],
      "source": [
        "strat_train_set, strat_test_set = strat_splits[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv3V4DJB6bVr"
      },
      "outputs": [],
      "source": [
        "strat_train_set, strat_test_set = train_test_split(\n",
        "    housing, test_size=0.2, stratify=housing[\"income_cat\"], random_state=42)\n",
        "\n",
        "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL3OHa_K6oGt"
      },
      "outputs": [],
      "source": [
        "def income_cat_proportions(data):\n",
        "    return data[\"income_cat\"].value_counts() / len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall %\": income_cat_proportions(housing),\n",
        "    \"Stratified %\": income_cat_proportions(strat_test_set),\n",
        "    \"Random %\": income_cat_proportions(test_set),\n",
        "}).sort_index()\n",
        "compare_props.index.name = \"Income Category\"\n",
        "compare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] /\n",
        "                                   compare_props[\"Overall %\"] - 1)\n",
        "compare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] /\n",
        "                                  compare_props[\"Overall %\"] - 1)\n",
        "(compare_props * 100).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OP-t32NY-s3"
      },
      "outputs": [],
      "source": [
        "for set_ in (strat_train_set, strat_test_set):\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYMVzMrpZEsH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8G4Ewq-aLU-"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiKVSQtYaLMD"
      },
      "outputs": [],
      "source": [
        "# bad_visualization_plot\n",
        "\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
        "plt.savefig(\"bad_visualization_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3kPTS-qaLFI"
      },
      "outputs": [],
      "source": [
        "# better_visualization_plot\n",
        "\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
        "plt.savefig(\"better_visualization_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO6WD8k8cSqx"
      },
      "outputs": [],
      "source": [
        "# housing_prices_scatterplot\n",
        "\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
        "             s=housing[\"population\"] / 100, label=\"population\",\n",
        "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
        "             legend=True, sharex=False, figsize=(10, 7))\n",
        "plt.savefig(\"housing_prices_scatterplot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ox6PocdfwgB"
      },
      "source": [
        "California housing prices: red is expensive, blue is cheap, larger circles indicate areas with a larger population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ4gQyCacaOM"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()\n",
        "\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxtMXqVsgcYd"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
        "plt.savefig(\"scatter_matrix_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyHoHi60i4n5"
      },
      "source": [
        "This scatter matrix plots every numerical attribute against every other numerical attribute, plus a histogram of each numerical attributeâ€™s values on the main diagonal (top left to bottom right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEdU4lddiSx4"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
        "             alpha=0.1, grid=True)\n",
        "plt.savefig(\"income_vs_house_value_scatterplot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYn_ZDRmLII"
      },
      "source": [
        "## Experimenting with Attribute Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAnNGjyPkWbY"
      },
      "outputs": [],
      "source": [
        "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
        "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
        "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU1spf58mRZU"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loxmZT-Tnnop"
      },
      "source": [
        "# Preparing Data For Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX0P79LamUM8"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
        "housing_labels = strat_train_set[\"median_house_value\"].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1BgHGKrn6T_"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEN2IfxJnthM"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE0aJscA2xbm"
      },
      "source": [
        "Separating out the numerical attributes to use the `\"median\"` strategy (as it cannot be calculated on text attributes like `ocean_proximity`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLG1rNEc2ywo"
      },
      "outputs": [],
      "source": [
        "housing_num = housing.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYZSu0-Z3kY8"
      },
      "outputs": [],
      "source": [
        "imputer.fit(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zcb07TME3u6O"
      },
      "outputs": [],
      "source": [
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jyohADp6IOH"
      },
      "outputs": [],
      "source": [
        "housing_num.median().values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEeDry296NxD"
      },
      "outputs": [],
      "source": [
        "X = imputer.transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgkJGQvC67Yk"
      },
      "outputs": [],
      "source": [
        "imputer.feature_names_in_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzP4hvC87Bgk"
      },
      "outputs": [],
      "source": [
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing_num.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6ltgxc877TU"
      },
      "outputs": [],
      "source": [
        "housing_tr.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj3V_b4Y8e6Z"
      },
      "outputs": [],
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDW-GRfx99V7"
      },
      "outputs": [],
      "source": [
        "np.unique(housing_cat.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bk6wcCQ9XQ_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_onehot = cat_encoder.fit_transform(housing_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxzyibkz91F4"
      },
      "outputs": [],
      "source": [
        "housing_cat_onehot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oOIAvz8BbgC"
      },
      "source": [
        "By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5pyAk5AIVLu"
      },
      "outputs": [],
      "source": [
        "housing_cat_onehot.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pInSYFncBwTa"
      },
      "source": [
        "Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nZvJWLwBlVG"
      },
      "outputs": [],
      "source": [
        "cat_encoder = OneHotEncoder(sparse=False)\n",
        "housing_cat_onehot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_jkX5d8DU-Z"
      },
      "outputs": [],
      "source": [
        "cat_encoder.categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzY2EZSoDaa6"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame({\"ocean_proximity\": ['INLAND', 'NEAR BAY']})\n",
        "pd.get_dummies(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOl83weVnBCC"
      },
      "outputs": [],
      "source": [
        "cat_encoder.transform(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHZFtCiMnJma"
      },
      "outputs": [],
      "source": [
        "df_test_unknown = pd.DataFrame({\"ocean_proximity\": ['<2H OCEAN', 'ISLAND']})\n",
        "pd.get_dummies(df_test_unknown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pegGtpGnvHG"
      },
      "outputs": [],
      "source": [
        "cat_encoder.handle_unknown = \"ignore\"\n",
        "cat_encoder.transform(df_test_unknown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI5SoTwioByZ"
      },
      "outputs": [],
      "source": [
        "cat_encoder.feature_names_in_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shqX484MoMMX"
      },
      "outputs": [],
      "source": [
        "cat_encoder.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7KFhhE4oTHg"
      },
      "outputs": [],
      "source": [
        "df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n",
        "                         columns=cat_encoder.get_feature_names_out(),\n",
        "                         index=df_test_unknown.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwWvn_5aoZ8N"
      },
      "outputs": [],
      "source": [
        "df_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VQOgm6cobU8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJPEQ1zEuYDh"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaled = StandardScaler()\n",
        "housing_num_std_scaled = std_scaled.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGQegyLygafA"
      },
      "outputs": [],
      "source": [
        "# extra code â€“ this cell generates Figure 2â€“17\n",
        "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
        "housing[\"population\"].hist(ax=axs[0], bins=50)\n",
        "housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
        "axs[0].set_xlabel(\"Population\")\n",
        "axs[1].set_xlabel(\"Log of population\")\n",
        "axs[0].set_ylabel(\"Number of districts\")\n",
        "plt.savefig(\"long_tail_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpOAp0rwg41P"
      },
      "source": [
        "What if we replace each value with its percentile?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mESjWo-dg9Uf"
      },
      "outputs": [],
      "source": [
        "# extra code â€“ just shows that we get a uniform distribution\n",
        "percentiles = [np.percentile(housing[\"median_income\"], p)\n",
        "               for p in range(1, 100)]\n",
        "flattened_median_income = pd.cut(housing[\"median_income\"],\n",
        "                                 bins=[-np.inf] + percentiles + [np.inf],\n",
        "                                 labels=range(1, 100 + 1))\n",
        "flattened_median_income.hist(bins=50)\n",
        "plt.xlabel(\"Median income percentile\")\n",
        "plt.ylabel(\"Number of districts\")\n",
        "plt.show()\n",
        "# Note: incomes below the 1st percentile are labeled 1, and incomes above the\n",
        "# 99th percentile are labeled 100. This is why the distribution below ranges\n",
        "# from 1 to 100 (not 0 to 100)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LNBVImFbL4x"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAjez2jHfYsG"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "target_scaler = StandardScaler()\n",
        "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(housing[[\"median_income\"]], scaled_labels)\n",
        "some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
        "\n",
        "scaled_predictions = model.predict(some_new_data)\n",
        "predictions = target_scaler.inverse_transform(scaled_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYi-TalIkg1S"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhLQtLCvqvm5"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "model = TransformedTargetRegressor(LinearRegression(),\n",
        "                                   transformer=StandardScaler())\n",
        "model.fit(housing[[\"median_income\"]], housing_labels)\n",
        "predictions = model.predict(some_new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zOY85gyt4c-"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0Tl3CGu-fR"
      },
      "source": [
        "### Custom Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUxn51UBvJQ-"
      },
      "source": [
        "To create simple transformers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmdccBjYt6Xs"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
        "log_pop = log_transformer.transform(housing[[\"population\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOIrAGqHvPYp"
      },
      "outputs": [],
      "source": [
        "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
        "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWTdOEDnvUni"
      },
      "outputs": [],
      "source": [
        "age_simil_35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj8iOtaOvDDT"
      },
      "outputs": [],
      "source": [
        "sf_coords = 37.7749, -122.41\n",
        "sf_transformer = FunctionTransformer(rbf_kernel,\n",
        "                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\n",
        "sf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7v4UmD7vWKo"
      },
      "outputs": [],
      "source": [
        "sf_simil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeiVcPk6vrfR"
      },
      "outputs": [],
      "source": [
        "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
        "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krvGatATvrbO"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.utils.validation import check_array, check_is_fitted\n",
        "\n",
        "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
        "        self.with_mean = with_mean\n",
        "\n",
        "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
        "        X = check_array(X)  # checks that X is an array with finite float values\n",
        "        self.mean_ = X.mean(axis=0)\n",
        "        self.scale_ = X.std(axis=0)\n",
        "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
        "        X = check_array(X)\n",
        "        assert self.n_features_in_ == X.shape[1]\n",
        "        if self.with_mean:\n",
        "            X = X - self.mean_\n",
        "        return X / self.scale_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lEYtFFsvyrR"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.gamma = gamma\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y=None, sample_weight=None):\n",
        "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
        "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
        "        return self  # always return self!\n",
        "\n",
        "    def transform(self, X):\n",
        "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
        "\n",
        "    def get_feature_names_out(self, names=None):\n",
        "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEn88DsSv2UY"
      },
      "outputs": [],
      "source": [
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "similarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n",
        "                                           sample_weight=housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N_nSP6gv544"
      },
      "outputs": [],
      "source": [
        "similarities[:3].round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGZJEUt0wW3I"
      },
      "outputs": [],
      "source": [
        "# extra code â€“ this cell generates Figure 2â€“19\n",
        "\n",
        "housing_renamed = housing.rename(columns={\n",
        "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
        "    \"population\": \"Population\",\n",
        "    \"median_house_value\": \"Median house value (á´œsá´…)\"})\n",
        "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
        "\n",
        "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
        "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
        "                     c=\"Max cluster similarity\",\n",
        "                     cmap=\"jet\", colorbar=True,\n",
        "                     legend=True, sharex=False, figsize=(10, 7))\n",
        "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
        "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
        "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
        "         label=\"Cluster centers\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(\"district_cluster_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgtmIQWGPiQh"
      },
      "source": [
        "### Transformation Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDAlt75ZPr8f"
      },
      "source": [
        "Now let's build a pipeline to preprocess the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hhHP0Y8Ts22"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"standardize\", StandardScaler()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXPQcmYrwzWG"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTsBCToMTv1N"
      },
      "outputs": [],
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "set_config(display = 'diagram')\n",
        "\n",
        "num_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mebd7aYPUK1I"
      },
      "outputs": [],
      "source": [
        "housing_num_prepared = num_pipeline.fit_transform(housing_num)\n",
        "housing_num_prepared[:3].round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lIf6TAXbfPO"
      },
      "outputs": [],
      "source": [
        "df_housing_num_prepared = pd.DataFrame(housing_num_prepared,\n",
        "                                       columns=num_pipeline.get_feature_names_out(), index = housing.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV8M6m-vcsGD"
      },
      "outputs": [],
      "source": [
        "df_housing_num_prepared.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vGAWUoec1nF"
      },
      "outputs": [],
      "source": [
        "num_pipeline.steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIYEskVpddcF"
      },
      "outputs": [],
      "source": [
        "num_pipeline[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_CsmeSmdiBk"
      },
      "outputs": [],
      "source": [
        "num_pipeline[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fwrKEkfdlEj"
      },
      "outputs": [],
      "source": [
        "num_pipeline.named_steps[\"simpleimputer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S812rnjWduf5"
      },
      "outputs": [],
      "source": [
        "num_pipeline.set_params(simpleimputer__strategy=\"median\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8eAG3eSdzL4"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
        "               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "cat_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"most_frequent\"),\n",
        "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "\n",
        "preprocessing = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, num_attribs),\n",
        "    (\"cat\", cat_pipeline, cat_attribs),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUWsdhmyeDMW"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_selector, make_column_transformer\n",
        "\n",
        "preprocessing = make_column_transformer(\n",
        "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
        "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAp18PWMgiJ1"
      },
      "outputs": [],
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l0HvrlajX2h"
      },
      "outputs": [],
      "source": [
        "def column_ratio(X):\n",
        "    return X[:, [0]] / X[:, [1]]\n",
        "\n",
        "def ratio_name(function_transformer, feature_names_in):\n",
        "    return [\"ratio\"]  # feature names out\n",
        "\n",
        "def ratio_pipeline():\n",
        "    return make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
        "        StandardScaler())\n",
        "\n",
        "log_pipeline = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
        "    StandardScaler())\n",
        "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
        "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
        "                                     StandardScaler())\n",
        "preprocessing = ColumnTransformer([\n",
        "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
        "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
        "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
        "        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
        "                               \"households\", \"median_income\"]),\n",
        "        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
        "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
        "    ],\n",
        "    remainder=default_num_pipeline)  # one column remaining: housing_median_age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hM4DSXotFnN"
      },
      "outputs": [],
      "source": [
        "housing_prepared = preprocessing.fit_transform(housing)\n",
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AMqpC6OtIj3"
      },
      "outputs": [],
      "source": [
        "preprocessing.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOPuILfE085N"
      },
      "source": [
        "### Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8DRjOue08DO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
        "lin_reg.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm-CsU7DtTVc"
      },
      "outputs": [],
      "source": [
        "housing_predictions = lin_reg.predict(housing)\n",
        "housing_predictions[:5].round(-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYm8gjYIBuFw"
      },
      "outputs": [],
      "source": [
        "housing_labels.iloc[:5].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HzfICk8CVe1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k2viPXjFtJL"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
        "tree_reg.fit(housing, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUc8sTMWWhzC"
      },
      "outputs": [],
      "source": [
        "housing_predictions = tree_reg.predict(housing)\n",
        "tree_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                              squared=False)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFiIt8qHXvQn"
      },
      "source": [
        "### Better Evaluation Using Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzDw8sJKWvco"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hilFGbKHZOR7"
      },
      "outputs": [],
      "source": [
        "pd.Series(tree_rmses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-az6d6Fbowx"
      },
      "outputs": [],
      "source": [
        "# extra code â€“ computes the error stats for the linear model\n",
        "lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,\n",
        "                              scoring=\"neg_root_mean_squared_error\", cv=10)\n",
        "pd.Series(lin_rmses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZXhrsgEj7Ly"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = make_pipeline(preprocessing,\n",
        "                           RandomForestRegressor(random_state=42))\n",
        "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
        "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcvQNBoikQVh"
      },
      "outputs": [],
      "source": [
        "pd.Series(forest_rmses).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare this RMSE measured using cross-validation (the \"validation error\") with the RMSE measured on the training set (the \"training error\"):"
      ],
      "metadata": {
        "id": "zxHfahH9V_sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_reg.fit(housing, housing_labels)\n",
        "housing_predictions = forest_reg.predict(housing)\n",
        "forest_rmse = mean_squared_error(housing_labels, housing_predictions,\n",
        "                                 squared=False)\n",
        "forest_rmse"
      ],
      "metadata": {
        "id": "6flemw1BUZ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training error is much lower than the validation error, which usually means that the model has overfit the training set. Another possible explanation may be that there's a mismatch between the training data and the validation data, but it's not the case here, since both came from the same dataset that we shuffled and split in two parts."
      ],
      "metadata": {
        "id": "xKhgjRF-Wf1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tune Your Model"
      ],
      "metadata": {
        "id": "EA2bZdkaXmBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search"
      ],
      "metadata": {
        "id": "whYUkf3OXtcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "full_pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessing),\n",
        "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
        "])\n",
        "param_grid = [\n",
        "    {'preprocessing__geo__n_clusters': [5, 8, 10],\n",
        "     'random_forest__max_features': [4, 6, 8]},\n",
        "    {'preprocessing__geo__n_clusters': [10, 15],\n",
        "     'random_forest__max_features': [6, 8, 10]},\n",
        "]\n",
        "grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n",
        "                           scoring='neg_root_mean_squared_error')\n",
        "grid_search.fit(housing, housing_labels)"
      ],
      "metadata": {
        "id": "m5VFWTmDWI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get the full list of hyperparameters available for tuning by looking at full_pipeline.get_params().keys():"
      ],
      "metadata": {
        "id": "zL2BBRu6fcEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code â€“ shows part of the output of get_params().keys()\n",
        "print(str(full_pipeline.get_params().keys())[:1000] + \"...\")"
      ],
      "metadata": {
        "id": "x7VoWFUWdk8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best hyperparameter combination found:"
      ],
      "metadata": {
        "id": "OF3xqkjSgkGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "IVrTNf1zgd80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "HRUaGCpTghXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the score of each hyperparameter combination tested during the grid search:"
      ],
      "metadata": {
        "id": "zqBsThlxgxOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
        "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
        "\n",
        "# extra code â€“ these few lines of code just make the DataFrame look nicer\n",
        "cv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n",
        "                 \"param_random_forest__max_features\", \"split0_test_score\",\n",
        "                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\n",
        "score_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\n",
        "cv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\n",
        "cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n",
        "\n",
        "cv_res.head()"
      ],
      "metadata": {
        "id": "mQSS9wi8gqb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus section: how to choose the sampling distribution for a hyperparameter**\n",
        "\n",
        "* `scipy.stats.randint(a, b+1)`: for hyperparameters with _discrete_ values that range from a to b, and all values in that range seem equally likely.\n",
        "* `scipy.stats.uniform(a, b)`: this is very similar, but for _continuous_ hyperparameters.\n",
        "* `scipy.stats.geom(1 / scale)`: for discrete values, when you want to sample roughly in a given scale. E.g., with scale=1000 most samples will be in this ballpark, but ~10% of all samples will be <100 and ~10% will be >2300.\n",
        "* `scipy.stats.expon(scale)`: this is the continuous equivalent of `geom`. Just set `scale` to the most likely value.\n",
        "* `scipy.stats.loguniform(a, b)`: when you have almost no idea what the optimal hyperparameter value's scale is. If you set a=0.01 and b=100, then you're just as likely to sample a value between 0.01 and 0.1 as a value between 10 and 100.\n"
      ],
      "metadata": {
        "id": "cTRBiBVXc-qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code â€“ plots a few distributions you can use in randomized search\n",
        "\n",
        "from scipy.stats import randint, uniform, geom, expon\n",
        "\n",
        "xs1 = np.arange(0, 7 + 1)\n",
        "randint_distrib = randint(0, 7 + 1).pmf(xs1)\n",
        "\n",
        "xs2 = np.linspace(0, 7, 500)\n",
        "uniform_distrib = uniform(0, 7).pdf(xs2)\n",
        "\n",
        "xs3 = np.arange(0, 7 + 1)\n",
        "geom_distrib = geom(0.5).pmf(xs3)\n",
        "\n",
        "xs4 = np.linspace(0, 7, 500)\n",
        "expon_distrib = expon(scale=1).pdf(xs4)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.bar(xs1, randint_distrib, label=\"scipy.randint(0, 7 + 1)\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.legend()\n",
        "plt.axis([-1, 8, 0, 0.2])\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.fill_between(xs2, uniform_distrib, label=\"scipy.uniform(0, 7)\")\n",
        "plt.ylabel(\"PDF\")\n",
        "plt.legend()\n",
        "plt.axis([-1, 8, 0, 0.2])\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(xs3, geom_distrib, label=\"scipy.geom(0.5)\")\n",
        "plt.xlabel(\"Hyperparameter value\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.legend()\n",
        "plt.axis([0, 7, 0, 1])\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.fill_between(xs4, expon_distrib, label=\"scipy.expon(scale=1)\")\n",
        "plt.xlabel(\"Hyperparameter value\")\n",
        "plt.ylabel(\"PDF\")\n",
        "plt.legend()\n",
        "plt.axis([0, 7, 0, 1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FH8Q6-Iuc3yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the PDF for `expon()` and `loguniform()` (left column), as well as the PDF of log(X) (right column). The right column shows the distribution of hyperparameter _scales_. You can see that `expon()` favors hyperparameters with roughly the desired scale, with a longer tail towards the smaller scales. But `loguniform()` does not favor any scale, they are all equally likely:"
      ],
      "metadata": {
        "id": "7nZPmgm8hQ9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code â€“ shows the difference between expon and loguniform\n",
        "\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "xs1 = np.linspace(0, 7, 500)\n",
        "expon_distrib = expon(scale=1).pdf(xs1)\n",
        "\n",
        "log_xs2 = np.linspace(-5, 3, 500)\n",
        "log_expon_distrib = np.exp(log_xs2 - np.exp(log_xs2))\n",
        "\n",
        "xs3 = np.linspace(0.001, 1000, 500)\n",
        "loguniform_distrib = loguniform(0.001, 1000).pdf(xs3)\n",
        "\n",
        "log_xs4 = np.linspace(np.log(0.001), np.log(1000), 500)\n",
        "log_loguniform_distrib = uniform(np.log(0.001), np.log(1000)).pdf(log_xs4)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.fill_between(xs1, expon_distrib,\n",
        "                 label=\"scipy.expon(scale=1)\")\n",
        "plt.ylabel(\"PDF\")\n",
        "plt.legend()\n",
        "plt.axis([0, 7, 0, 1])\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.fill_between(log_xs2, log_expon_distrib,\n",
        "                 label=\"log(X) with X ~ expon\")\n",
        "plt.legend()\n",
        "plt.axis([-5, 3, 0, 1])\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.fill_between(xs3, loguniform_distrib,\n",
        "                 label=\"scipy.loguniform(0.001, 1000)\")\n",
        "plt.xlabel(\"Hyperparameter value\")\n",
        "plt.ylabel(\"PDF\")\n",
        "plt.legend()\n",
        "plt.axis([0.001, 1000, 0, 0.005])\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.fill_between(log_xs4, log_loguniform_distrib,\n",
        "                 label=\"log(X) with X ~ loguniform\")\n",
        "plt.xlabel(\"Log of hyperparameter value\")\n",
        "plt.legend()\n",
        "plt.axis([-8, 1, 0, 0.2])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0EmMDao-g1bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the Best Models and their Error"
      ],
      "metadata": {
        "id": "UyjAQdXLhjbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "final_model = rnd_search.best_estimator_  # includes preprocessing\n",
        "feature_importances = final_model[\"random_forest\"].feature_importances_\n",
        "feature_importances.round(2)"
      ],
      "metadata": {
        "id": "sE5OGnRFhM-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_"
      ],
      "metadata": {
        "id": "9SbmvMAEhsHn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyON3SwMKfSAsVq/BOe/doUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}